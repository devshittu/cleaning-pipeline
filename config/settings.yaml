# config/settings.yaml

general:
  log_level: INFO # Set to INFO for production readiness, DEBUG for development
  gpu_enabled: True # Set to True to leverage your RTX A4000

ingestion_service:
  port: 8000
  model_name: "en_core_web_trf" # The spaCy model to use for NER. Change if needed.
  model_cache_dir: "/app/.cache/spacy" # Path for spaCy to cache models.
  dateparser_languages: ["en"] # Languages for dateparser to consider.
  batch_processing_threads: 2 # Number of threads for CLI batch processing (will be less relevant with Celery)

celery:
  broker_url: "redis://redis:6379/0" # Redis as broker
  result_backend: "redis://redis:6379/0" # Redis as result backend
  task_acks_late: True # Acknowledge task only after it's done
  worker_prefetch_multiplier: 1 # Only fetch one task at a time per worker process
  worker_concurrency: 1 # Number of worker processes. Adjust based on CPU cores (e.g., 4-8 for Threadripper)
  # For GPU, often 1 process per GPU, but spaCy might benefit from multiple threads within that process.
  # We'll start with processes and refine if needed.
  task_annotations:
    '*':
      rate_limit: '300/m' # Example rate limit: 300 tasks per minute globally

# New storage configuration
storage:
  backend: "jsonl" # Default storage backend: jsonl, elasticsearch, or postgresql
  
  jsonl:
    output_path: "/app/data/processed_articles.jsonl" # Default output path for JSONL

  elasticsearch:
    host: "elasticsearch" # Elasticsearch host (Docker service name or IP)
    port: 9200
    scheme: "http"
    index_name: "news_articles"
    api_key: null # Placeholder: Use environment variables for production (e.g., ES_API_KEY)
    # basic_auth: ["user", "password"] # Example for basic auth, prefer API key

  postgresql:
    host: "postgres" # PostgreSQL host (Docker service name or IP)
    port: 5432
    dbname: "newsdb"
    user: "user" # Placeholder: Use environment variables for production (e.g., PG_USER)
    password: "password" # Placeholder: Use environment variables for production (e.g., PG_PASSWORD)
    table_name: "processed_articles"


logging:
  version: 1
  disable_existing_loggers: False
  formatters:
    json:
      class: pythonjsonlogger.jsonlogger.JsonFormatter
      format: "%(levelname)s %(asctime)s %(filename)s %(funcName)s %(lineno)d %(message)s"
  handlers:
    console:
      class: logging.StreamHandler
      formatter: json
      stream: ext://sys.stdout
    ingestion_file:
      class: logging.handlers.RotatingFileHandler
      formatter: json
      filename: /app/logs/ingestion_service.jsonl
      maxBytes: 10485760 # 10 MB
      backupCount: 5
  root:
    handlers: [console]
    level: INFO
  loggers:
    ingestion_service:
      handlers: [ingestion_file, console]
      level: INFO
      propagate: False

# # config/settings.yaml

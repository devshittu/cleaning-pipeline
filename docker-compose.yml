# docker-compose.yml
services:
  ingestion-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ingestion-service
    ports:
      - "8000:8000"
    volumes:
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./data:/app/data:rw # For CLI batch processing
      - ./.cache:/app/.cache # Persist spaCy model cache
    environment:
      PYTHONUNBUFFERED: "1"
      LOG_LEVEL: INFO # Can be overridden by settings.yaml
      # Pass Celery broker URL as env var for explicit config
      CELERY_BROKER_URL: "redis://redis:6379/0"
      CELERY_RESULT_BACKEND: "redis://redis:6379/0"
    deploy:
      resources:
        reservations:
          devices:
            # Uncomment the following lines if you need GPU support for spaCy
            # This requires the NVIDIA Container Toolkit to be installed.
            - capabilities: [gpu]
              count: all
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      - redis # Ensure Redis is up before ingestion-service

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data # Persist Redis data
    command: redis-server --appendonly yes
    restart: always

  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: celery-worker
    # Changed --pool=fork to --pool=prefork for Celery 5.x compatibility.
    # 'prefork' is the default process-based pool.
    command: celery -A src.celery_app worker --loglevel=DEBUG --pool=prefork --concurrency=1
    # Use --pool=solo or --pool=prefork with concurrency=1 if GPU is used per-process
    # For Threadripper, we can use multiple processes (--concurrency)
    volumes:
      - ./data:/app/data:rw # Explicitly mark as read-write
      - ./config:/app/config:ro
      - ./logs:/app/logs
      - ./.cache:/app/.cache # Persist spaCy model cache
    environment:
      PYTHONUNBUFFERED: "1"
      LOG_LEVEL: INFO
      # Pass Celery broker URL as env var for explicit config
      CELERY_BROKER_URL: "redis://redis:6379/0"
      CELERY_RESULT_BACKEND: "redis://redis:6379/0"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              count: all # Allow Celery workers to use GPUs
    depends_on:
      - redis # Ensure Redis is up before Celery workers

  # Uncomment if using Elasticsearch
  # elasticsearch:
  #   image: docker.elastic.co/elasticsearch/elasticsearch:8.14.0
  #   container_name: elasticsearch
  #   environment:
  #     - xpack.security.enabled=false # For simplicity in testing, disable security
  #     - discovery.type=single-node
  #   ports:
  #     - "9200:9200"
  #     - "9300:9300"
  #   volumes:
  #     - es_data:/usr/share/elasticsearch/data
  #   restart: on-failure

  # Uncomment if using PostgreSQL
  # postgres:
  #   image: postgres:16.3-alpine
  #   container_name: postgres
  #   environment:
  #     POSTGRES_DB: newsdb
  #     POSTGRES_USER: user
  #     POSTGRES_PASSWORD: password
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - pg_data:/var/lib/postgresql/data
  #   restart: on-failure
volumes:
  redis_data:
  es_data:
  pg_data:
  data: # Added named volume for persistence
